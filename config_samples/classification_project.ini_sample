# This is a configuration template for a CLASSIFICATION (segmentation) study.
[metadata]

# Project type
project_type = classification

# Configuration file version
version = 0

[settings]

# Trainer mode: one of `train`, `resume`, `predict`
trainer_mode = train

# Model class (one of unet, attention_unet, swin_unetr, dynunet)
model_class = unet

# Is the dataset 3D?
is_3d = False

# Resample dataset to isotropic resolution? Only relevant if `is_3d = True`
to_isotropic = False

# Up-scale z to xy resolution? Only relevant if `is_3d = True`
up_scale_z = False

# Voxel size (z, y, x: used for resampling). Only relevant if `is_3d = True`
voxel_size =

# Root project directory
project_dir = /path/to/project

# Root of data directory: either absolute path, or relative to `project_dir`
data_dir = ground_truth

# Subfolder for the source images for training/validation/testing
source_images_sub_folder = images

# Subfolder for the target images for training/validation/testing
target_images_sub_folder = labels

# Label for the source images for training/validation/testing
source_images_label = image

# Label for the target images for training/validation/testing
target_images_label = label

# Number of input channels (e.g., 1 for a gray-scale image)
in_channels = 1

# Number of output channels (corresponds to the number of classes to predict)
out_channels = 3

# Fraction of source images to be used for training (0.0 - 1.0): omit for default
train_fraction = 0.7

# Fraction of source images to be used for validation (0.0 - 1.0): omit for default
val_fraction = 0.2

# Fraction of source images to be used for testing (0.0 - 1.0): omit for default
test_fraction = 0.1

# Checkpoint monitor: one of "loss" or "metrics"
checkpoint_monitor = loss

# Checkpoint metrics class: if `checkpoint_target` is `metrics`, one of the classes from `class_names`
# can be used as monitor. Leave unset to use the global validation metric.
# Ignored if `checkpoint_target` is loss.
checkpoint_metrics_class =

# Use early stopping based on `checkpoint_monitor` and `checkpoint_metrics_class`?
use_early_stopping = True
early_stopping_patience = 10

# Full path to the images to be used for prediction: must be specified if trainer_mode is `predict`
source_for_prediction = 

# Full path to the folder where the predicted images will be stored: omit for default target
target_for_prediction =

# Full path to an existing model for `resume` or `predict`: ignored if trainer_mode is `train`
# In case of ensemble prediction, pass the path to the models folder that contains the
# various folds.
source_model_path =

# Seed for random number generation initialization
seed = 2022

# Batch size for training
batch_size = 8

# Batch size for interference
inference_batch_size = 4

# Number of patches per image: total batch size will be `batch_size` * `num_patches`
num_patches = 1

# [unet] Number of residual units for the U-Net
num_res_units = 4

# [unet, attenuation_net] Number of layers and corresponding filters in the U-Net encoder (contracting path)
channels = 16, 32, 64

# [unet, attenuation_net] Kernel strides: must be one entry shorter than `channels` (omit to use defaults)
strides = 2, 2

# [swin_unetr] Number of layers in each stage
depths = 2, 2, 2, 2

# [swin_unetr] Number of attention heads
num_heads = 3, 6, 12, 24

# [swin_unetr] Dimension of network feature size
feature_size = 24

# [swin_unetr] Whether to use v2 architecture
use_v2 = False

# [dynunet] Whether to add deep supervision head before output
deep_supervision = False

# [dynunet] Number of feature maps output during deep supervision
# Ignored if deep_supervision is False
deep_supr_num = 1

# [dynunet] Whether to use residual connection based convolution blocks
res_block = True

# Size of one patch
patch_size = 640, 640

# Initial learning rate
learning_rate = 0.001

# Whether the background class should be included in the calculation of loss and metrics
include_background = True

# Class names (for logs and TensorBoard)
class_names =  background, cell, membrane

# Maximum number of training epochs
max_epochs = 2000

# Output data type for full inference
output_dtype = int32

# Precision used by PyTorch for calculations
# Please see https://lightning.ai/docs/pytorch/stable/common/precision_basic.html
precision = 16-mixed
