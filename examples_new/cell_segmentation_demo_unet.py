# ******************************************************************************
# Copyright Â© 2022 - 2024, ETH Zurich, D-BSSE, Aaron Ponti
# All rights reserved. This program and the accompanying materials
# are made available under the terms of the Apache License Version 2.0
# which accompanies this distribution, and is available at
# https://www.apache.org/licenses/LICENSE-2.0.txt
#
# Contributors:
#   Aaron Ponti - initial API and implementation
# ******************************************************************************

import sys
from pathlib import Path

import pytorch_lightning as pl
from monai.losses import DiceCELoss
from monai.metrics import DiceMetric
from pytorch_lightning.callbacks import (
    EarlyStopping,
    LearningRateMonitor,
    ModelCheckpoint,
)
from torch.optim.lr_scheduler import OneCycleLR

from qute import device
from qute.campaigns import SegmentationCampaignTransforms2D
from qute.config import Config
from qute.data.dataloaders import DataModuleLocalFolder
from qute.data.demos import CellSegmentationDemo
from qute.mode import TrainerMode
from qute.models.unet import UNet
from qute.project import Project
from qute.random import set_global_rng_seed

if __name__ == "__main__":

    # Read the configuration file
    CONFIG = Config(Path(__file__).parent / "cell_segmentation_demo_unet_config.ini")
    if not CONFIG.parse():
        sys.exit("Could not parse configuration file!")

    # Initialize Project structure
    project = Project(CONFIG, clean=True)

    # Set trainer mode via configuration file:
    #    * trainer_mode = "train" (:= TrainerMode.TRAIN) to train
    #
    #    * trainer_mode = "resume" (:= TrainerMode.RESUME) to resume
    #        - CONFIG.source_model_path must point to a model checkpoint generated by training
    #
    #    * trainer_mode = "predict" (:= TrainerMode.PREDICT) to run full inference on a folder of images
    #        - CONFIG.source_model_path must point to a model checkpoint generated by training
    #        - CONFIG.source_for_prediction must point to a folder of containing source images for prediction
    #        - CONFIG.source_for_prediction may point to the target folder for prediction (otherwise, it will
    #              default to CONFIG.project_dir / "predictions" / ${RUN}
    trainer_mode = CONFIG.trainer_mode

    # Fine-tuning from self-supervised training vs. segmentation training from scratch:
    # if CONFIG.fine_tune_from_self_supervised is True, the model pointed at by CONFIG.source_model_path
    # is the one created by self-supervised/train_self_supervised.py, whose output layers will have
    # to be modified to predict the number of classes defined by CONFIG.out_channels.

    if __name__ == "__main__":
        # Seeding
        set_global_rng_seed(CONFIG.seed, workers=True)

        # Initialize default, example Segmentation Campaign Transform
        campaign_transforms = SegmentationCampaignTransforms2D(
            num_classes=CONFIG.out_channels,
            patch_size=CONFIG.patch_size,
            num_patches=CONFIG.num_patches,
        )

        # Data module
        data_module = CellSegmentationDemo(
            campaign_transforms=campaign_transforms,
            download_dir=CONFIG.project_dir,
            seed=CONFIG.seed,
            batch_size=CONFIG.batch_size,
            patch_size=CONFIG.patch_size,
            num_patches=CONFIG.num_patches,
            train_fraction=CONFIG.train_fraction,
            val_fraction=CONFIG.val_fraction,
            test_fraction=CONFIG.test_fraction,
            inference_batch_size=CONFIG.inference_batch_size,
        )

        # Train or continue training?
        if trainer_mode == TrainerMode.TRAIN or trainer_mode == TrainerMode.RESUME:

            # Inform
            print(f"Working directory: {project.run_dir}")

            # Calculate the number of steps per epoch
            data_module.prepare_data()
            data_module.setup("train")
            steps_per_epoch = len(data_module.train_dataloader())

            # Print the train, validation and test sets
            data_module.print_sets()

            # Loss
            criterion = DiceCELoss(
                include_background=CONFIG.include_background,
                to_onehot_y=False,
                softmax=True,
            )

            # Metrics
            metrics = DiceMetric(
                include_background=CONFIG.include_background,
                reduction="mean_batch",
                get_not_nans=False,
            )

            # Learning rate scheduler
            lr_scheduler_class = OneCycleLR
            lr_scheduler_parameters = {
                "total_steps": steps_per_epoch * CONFIG.max_epochs,
                "div_factor": 5.0,
                "max_lr": CONFIG.learning_rate,
                "pct_start": 0.5,
                # Fraction of total_steps at which the learning rate starts decaying after reaching max_lr
                "anneal_strategy": "cos",
            }

            # Callbacks
            early_stopping = EarlyStopping(
                monitor="val_loss", patience=10, mode="min"
            )  # Issues with Lightning's ES
            model_checkpoint = ModelCheckpoint(
                dirpath=project.models_dir, monitor="val_loss"
            )
            lr_monitor = LearningRateMonitor(logging_interval="step")

            # Instantiate the Trainer
            trainer = pl.Trainer(
                default_root_dir=project.results_dir,
                accelerator=device.get_accelerator(),
                devices=1,
                precision=CONFIG.precision,
                # callbacks=[model_checkpoint, early_stopping, lr_monitor],
                callbacks=[model_checkpoint, lr_monitor],
                max_epochs=CONFIG.max_epochs,
                log_every_n_steps=1,
            )

            # Store parameters
            # trainer.hparams = { }
            trainer.logger._default_hp_metric = False

            # Model
            if trainer_mode == TrainerMode.TRAIN:

                # Train for segmentation from scratch
                model = UNet(
                    campaign_transforms=campaign_transforms,
                    in_channels=CONFIG.in_channels,
                    out_channels=CONFIG.out_channels,
                    class_names=CONFIG.class_names,
                    num_res_units=CONFIG.num_res_units,
                    criterion=criterion,
                    channels=CONFIG.channels,
                    strides=CONFIG.strides,
                    metrics=metrics,
                    learning_rate=CONFIG.learning_rate,
                    lr_scheduler_class=lr_scheduler_class,
                    lr_scheduler_parameters=lr_scheduler_parameters,
                )

            elif trainer_mode == TrainerMode.RESUME:

                if CONFIG.fine_tune_from_self_supervised:
                    # Load existing model for continuing training (now with a different number of output channels)
                    model = UNet.load_from_checkpoint_and_swap_output_layer(
                        checkpoint_path=project.selected_model_path,
                        new_out_channels=CONFIG.out_channels,
                        class_names=CONFIG.class_names,
                        previous_out_channels=1,
                    )
                else:
                    # Load existing model: this model is a classification one, so there is no need to change
                    # the output layer.
                    model = UNet.load_from_checkpoint(project.selected_model_path)

                # Inform
                print(
                    f"Continuing training with existing model {project.selected_model_path}"
                )

            else:
                raise ValueError(f"Unrecognized trainer_mode: {trainer_mode}")

            # Train
            trainer.fit(model, datamodule=data_module)

            # Print path to best model
            print(f"Best model: {model_checkpoint.best_model_path}")

            # Set it into the project
            project.selected_model_path = model_checkpoint.best_model_path

            # Re-load weights from best model: this model is a classification one,
            # so there is no need to change the output layer.
            model = UNet.load_from_checkpoint(project.selected_model_path)

            # Test
            trainer.test(model, dataloaders=data_module.test_dataloader())

            # Predict on the test dataset
            predictions = trainer.predict(
                model, dataloaders=data_module.test_dataloader()
            )

        # Do we want to run a full inference?
        if trainer_mode == TrainerMode.PREDICT:
            if project.selected_model_path is None:
                raise ValueError("No model selected!")

            if CONFIG.fine_tune_from_self_supervised:
                raise ValueError(
                    "Please do not use a self-supervised model for prediction!"
                )

            # Load existing model: this model is a classification one, so there is no need to add
            # a new output layer for classification.
            model = UNet.load_from_checkpoint(project.selected_model_path)

        # Save the full predictions
        if project.source_for_prediction is None:
            print("No source folder provided for prediction. Quitting.")

            # Inform
            print(f"Predicting with existing model {project.selected_model_path}")

        else:
            model.full_inference(
                data_loader=data_module.inference_dataloader(
                    project.source_for_prediction
                ),
                target_folder=project.target_for_prediction,
                roi_size=CONFIG.patch_size,
                batch_size=CONFIG.inference_batch_size,
                transpose=False,
            )

        sys.exit(0)
